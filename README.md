### Proyecto NLP - AnÃ¡lisis de Sentimientos  

Este proyecto aborda el anÃ¡lisis de sentimientos utilizando mÃºltiples enfoques, incluyendo modelos tradicionales como **TF-IDF con Logistic Regression**, **CountVectorizer con SVC**, y un modelo neuronal basado en **GRU**. Se emplean tÃ©cnicas de preprocesamiento avanzadas con **spaCy, NLTK y regex**, asÃ­ como estrategias para mitigar el desbalanceo de clases mediante **SMOTETomek**.

#### ğŸ“‚ Estructura del Proyecto  

```bash
ğŸ“¦ NLP Sentiment Analysis
â”‚â”€â”€ ğŸ“„ app_features.py         # ExploraciÃ³n de datos (EDA), limpieza y anÃ¡lisis de texto
â”‚â”€â”€ ğŸ“„ app_extractor.py        # Preprocesamiento con spaCy, NLTK, regex y contractions
â”‚â”€â”€ ğŸ“„ app_utils.py            # Funciones auxiliares (tokenizaciÃ³n BERT, histogramas, etc.)
â”‚â”€â”€ ğŸ“„ app_model.py            # ImplementaciÃ³n de modelos clÃ¡sicos (TF-IDF, SVC, etc.)
â”‚â”€â”€ ğŸ“„ app_word2vec.py         # ImplementaciÃ³n de Word2Vec y visualizaciÃ³n de embeddings
â”‚â”€â”€ ğŸ“„ app_dataset.py          # Carga de embeddings precomputados para DeepModelGRU
â”‚â”€â”€ ğŸ“„ app_training.py         # Entrenamiento de DeepModelGRU con PyTorch y tÃ©cnicas avanzadas
â”‚â”€â”€ ğŸ“‚ datasets/               # Conjunto de datos utilizado en el anÃ¡lisis
â”‚â”€â”€ ğŸ“‚ embeddings/             # Embeddings precomputados almacenados
â”‚â”€â”€ ğŸ“‚ model/                  # Modelos entrenados y guardados (MModel, DeepModelGRU, Word2Vec)
â”‚â”€â”€ ğŸ“„ requirements.txt        # LibrerÃ­as necesarias para ejecutar el proyecto
```

#### ğŸ› ï¸ DescripciÃ³n de los MÃ³dulos  

### ğŸ”¹ `app_features.py`  
AnÃ¡lisis exploratorio de datos (**EDA**) con limpieza y preprocesamiento. Se examinan los **n-gramas mÃ¡s frecuentes**, la distribuciÃ³n de reseÃ±as, y se visualizan nubes de palabras de **sentimientos positivos y negativos**.  

### ğŸ”¹ `app_extractor.py`  
Contiene las clases `FeatureExtractor` y `PreProcess`, responsables del preprocesamiento textual. Se utilizan **spaCy y NLTK** para tokenizaciÃ³n y eliminaciÃ³n de **stopwords**, **contractions** para expandir contracciones, y **expresiones regulares** para limpiar texto.  

### ğŸ”¹ `app_utils.py`  
Funciones auxiliares como **precomputaciÃ³n de embeddings** con **BERT-base-uncased**, generaciÃ³n de histogramas y grÃ¡ficos para evaluar distribuciones de palabras y clases.  

### ğŸ”¹ `app_model.py`  
ImplementaciÃ³n de **MModel**, que ejecuta un pipeline con **TF-IDF + Logistic Regression** y **CountVectorizer + SVC**, utilizando **SMOTETomek** para mitigar el desbalanceo. TambiÃ©n se integra **ImbPipeline** para mejorar la representaciÃ³n de la clase minoritaria.  

### ğŸ”¹ `app_word2vec.py`  
Modelo **Word2Vec** con una puntuaciÃ³n de **0.9097**. Se visualizan embeddings en **2D con t-SNE**, se analiza la **cardinalidad del vocabulario** y se presentan ejemplos de palabras similares.  

### ğŸ”¹ `app_dataset.py`  
Clase **Dataset** para `DeepModelGRU`, encargada de cargar los **embeddings precomputados** y devolver los tensores listos para entrenamiento.  

### ğŸ”¹ `app_training.py`  
Clase de entrenamiento para `DeepModelGRU`, implementando **PyTorch** con:
- **BCEWithLogitsLoss** como funciÃ³n de pÃ©rdida.  
- **AdamW** como optimizador para mejorar la convergencia.  
- **Scheduler con paciencia de 2** para ajustar dinÃ¡micamente el learning rate.  
- **GradScaler** para manejar cÃ¡lculos en **punto flotante de precisiÃ³n mixta**, optimizando rendimiento en **GPU**.  

### ğŸ”¹ `datasets/`  
Carpeta que contiene los **conjuntos de datos** utilizados en el anÃ¡lisis y entrenamiento.  

### ğŸ”¹ `embeddings/`  
Almacena los **embeddings precomputados** generados con **BERT y Word2Vec**.  

### ğŸ”¹ `model/`  
Modelos entrenados y guardados, incluyendo **MModel, DeepModelGRU y Word2Vec**.  

### ğŸ”¹ `requirements.txt`  
Lista de librerÃ­as necesarias para ejecutar el proyecto sin conflictos.  
